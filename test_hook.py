import torch  
from transformers import AutoModelForCausalLM, AutoTokenizer  
  
# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨  
model = AutoModelForCausalLM.from_pretrained(  
    "/home/mint/Projects/ZipMoE/framework/models/deepseek-moe-16b-base",  
    trust_remote_code=True,  
    torch_dtype=torch.float16  
)  
tokenizer = AutoTokenizer.from_pretrained(  
    "/home/mint/Projects/ZipMoE/framework/models/deepseek-moe-16b-base",  
    trust_remote_code=True  
)  
  
class DeepSeekMoEHookTester:  
    """ä¸“é—¨ç”¨äºæµ‹è¯• DeepSeek MoE é—¨æ§é’©å­çš„ç±»"""  
      
    def __init__(self, model):  
        self.model = model  
        self.hooks = []  
        self.expert_selections = {}  
        self.gate_outputs = {}  
          
    def setup_gate_hooks(self):  
        """ä¸º DeepSeek MoE é—¨æ§å±‚è®¾ç½®é’©å­"""  
        for name, module in self.model.named_modules():  
            # ç²¾ç¡®åŒ¹é… DeepSeek MoE é—¨æ§  
            if (name.endswith('.mlp.gate') and   
                not name.endswith('.mlp.gate_proj') and  
                hasattr(module, '__class__') and   
                'Gate' in module.__class__.__name__):  
                  
                print(f"[SETUP] æ‰¾åˆ° DeepSeek MoE é—¨æ§: {name} ({type(module).__name__})")  
                self._add_gate_hook(module, name)  
                  
    def _add_gate_hook(self, module, name):  
        """ä¸ºé—¨æ§å±‚æ·»åŠ é’©å­"""  
        def gate_forward_hook(module, input, output):  
            print(f"[HOOK] ğŸ”¥ é—¨æ§é’©å­è§¦å‘: {name}")  
              
            try:  
                # å¤„ç† DeepSeek MoEGate è¾“å‡ºæ ¼å¼: (topk_idx, topk_weight, aux_loss)  
                if isinstance(output, tuple) and len(output) >= 2:  
                    topk_idx = output[0]  # ä¸“å®¶ç´¢å¼•  
                    topk_weight = output[1]  # ä¸“å®¶æƒé‡  
                    aux_loss = output[2] if len(output) > 2 else None  
                      
                    print(f"[HOOK] è¾“å‡ºæ ¼å¼: tuple, é•¿åº¦={len(output)}")  
                    print(f"[HOOK] topk_idx å½¢çŠ¶: {topk_idx.shape}, ç±»å‹: {topk_idx.dtype}")  
                    print(f"[HOOK] topk_weight å½¢çŠ¶: {topk_weight.shape}")  
                      
                    if torch.is_tensor(topk_idx) and topk_idx.dtype in [torch.int32, torch.int64, torch.long]:  
                        # è½¬æ¢ä¸º CPU å¹¶æå–ä¸“å®¶ç´¢å¼•  
                        expert_indices = topk_idx.cpu().tolist()  
                          
                        # å±•å¹³äºŒç»´åˆ—è¡¨å¹¶å»é‡  
                        if len(expert_indices) > 0 and isinstance(expert_indices[0], list):  
                            flattened_indices = []  
                            for token_experts in expert_indices:  
                                if isinstance(token_experts, list):  
                                    flattened_indices.extend(token_experts)  
                                else:  
                                    flattened_indices.append(token_experts)  
                            unique_experts = sorted(list(set(flattened_indices)))  
                        else:  
                            unique_experts = sorted(list(set(expert_indices)))  
                          
                        # å­˜å‚¨ç»“æœ  
                        self.expert_selections[name] = unique_experts  
                        self.gate_outputs[name] = {  
                            'topk_idx': topk_idx.cpu(),  
                            'topk_weight': topk_weight.cpu(),  
                            'aux_loss': aux_loss.cpu() if aux_loss is not None else None,  
                            'unique_experts': unique_experts  
                        }  
                          
                        print(f"[HOOK] {name}: æ¿€æ´»ä¸“å®¶ {unique_experts}")  
                        print(f"[HOOK] æ¿€æ´»ä¸“å®¶æ•°é‡: {len(unique_experts)}")  
                          
                        # éªŒè¯ä¸“å®¶ç´¢å¼•èŒƒå›´  
                        if hasattr(module, 'n_routed_experts'):  
                            n_experts = module.n_routed_experts  
                            if unique_experts and (max(unique_experts) >= n_experts or min(unique_experts) < 0):  
                                print(f"[HOOK] âš ï¸ è­¦å‘Š: ä¸“å®¶ç´¢å¼•è¶…å‡ºèŒƒå›´ [0, {n_experts-1}]")  
                          
                    else:  
                        print(f"[HOOK] âš ï¸ topk_idx ä¸æ˜¯æ•´æ•°å¼ é‡: {type(topk_idx)}")  
                          
                else:  
                    print(f"[HOOK] âš ï¸ æ„å¤–çš„è¾“å‡ºæ ¼å¼: {type(output)}")  
                    if isinstance(output, tuple):  
                        print(f"[HOOK] å…ƒç»„é•¿åº¦: {len(output)}")  
                        for i, item in enumerate(output):  
                            print(f"[HOOK] å…ƒç´  {i}: {type(item)}")  
                              
            except Exception as e:  
                print(f"[HOOK] âŒ å¤„ç†é—¨æ§è¾“å‡ºæ—¶å‡ºé”™: {e}")  
                import traceback  
                traceback.print_exc()  
                  
        # æ³¨å†Œé’©å­  
        hook_handle = module.register_forward_hook(gate_forward_hook)  
        self.hooks.append(hook_handle)  
        print(f"[SETUP] âœ… ä¸º {name} æ³¨å†Œé’©å­æˆåŠŸ")  
          
    def cleanup_hooks(self):  
        """æ¸…ç†æ‰€æœ‰é’©å­"""  
        for hook in self.hooks:  
            hook.remove()  
        print(f"[CLEANUP] æ¸…ç†äº† {len(self.hooks)} ä¸ªé’©å­")  
        self.hooks.clear()  
          
    def get_results(self):  
        """è·å–é’©å­æ•è·çš„ç»“æœ"""  
        return {  
            'expert_selections': self.expert_selections,  
            'gate_outputs': self.gate_outputs  
        }  
  
# æµ‹è¯•ä»£ç   
def test_deepseek_moe_hooks():  
    print("=== DeepSeek MoE é—¨æ§é’©å­æµ‹è¯• ===")  
      
    # åˆ›å»ºé’©å­æµ‹è¯•å™¨  
    hook_tester = DeepSeekMoEHookTester(model)  
      
    # è®¾ç½®é’©å­  
    print("\n=== è®¾ç½®é—¨æ§é’©å­ ===")  
    hook_tester.setup_gate_hooks()  
      
    if not hook_tester.hooks:  
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•é—¨æ§å±‚ï¼Œæ‰“å°æ‰€æœ‰åŒ…å« 'gate' çš„æ¨¡å—ï¼š")  
        for name, module in model.named_modules():  
            if 'gate' in name.lower():  
                print(f"  {name} -> {type(module).__name__}")  
        return  
      
    # å‡†å¤‡æµ‹è¯•è¾“å…¥  
    test_text = "ç”¨ä¸­æ–‡è®²ä¸€ä¸‹pythoné‡Œloggingçš„ç”¨æ³•"  
    inputs = tokenizer(test_text, return_tensors="pt")  
      
    print(f"\n=== æ‰§è¡Œå‰å‘ä¼ æ’­ ===")  
    print(f"è¾“å…¥æ–‡æœ¬: {test_text}")  
    print(f"è¾“å…¥ token æ•°é‡: {inputs.input_ids.shape[1]}")  
      
    try:  
        with torch.no_grad():  
            # æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œè§¦å‘é’©å­  
            outputs = model(**inputs)  
              
        print(f"\n=== é’©å­æ•è·ç»“æœ ===")  
        results = hook_tester.get_results()  
          
        for gate_name, experts in results['expert_selections'].items():  
            print(f"\né—¨æ§å±‚: {gate_name}")  
            print(f"  æ¿€æ´»çš„ä¸“å®¶: {experts}")  
            print(f"  ä¸“å®¶æ•°é‡: {len(experts)}")  
              
            if gate_name in results['gate_outputs']:  
                gate_info = results['gate_outputs'][gate_name]  
                print(f"  topk_idx å½¢çŠ¶: {gate_info['topk_idx'].shape}")  
                print(f"  topk_weight å½¢çŠ¶: {gate_info['topk_weight'].shape}")  
                if gate_info['aux_loss'] is not None:  
                    print(f"  aux_loss: {gate_info['aux_loss']}")  
                      
    except Exception as e:  
        print(f"âŒ å‰å‘ä¼ æ’­å‡ºé”™: {e}")  
        import traceback  
        traceback.print_exc()  
          
    finally:  
        # æ¸…ç†é’©å­  
        print(f"\n=== æ¸…ç†é’©å­ ===")  
        hook_tester.cleanup_hooks()  
  
if __name__ == "__main__":  
    test_deepseek_moe_hooks()